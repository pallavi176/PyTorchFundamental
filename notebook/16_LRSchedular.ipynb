{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AI\\NLP\\MyPractise\\PyTorchFundamental\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.0, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'initial_lr': 0.1, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "lambda1 = lambda epoch: epoch/10\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lambda1)\n",
    "\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010000000000000002\n",
      "0.020000000000000004\n",
      "0.03\n",
      "0.04000000000000001\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "# In this example, our learning rate is increasing, ideally it should be decreasing\n",
    "for epoch in range(5):\n",
    "    # loss.backward()\n",
    "    optimizer.step()\n",
    "    # validate(...)\n",
    "    scheduler.step()\n",
    "    print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative LR\n",
    "- Multiply the learning rate of each parameter group by the factor given in the specified function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False, 'initial_lr': 0.1, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "lambda1 = lambda epoch: 0.95\n",
    "scheduler = lr_scheduler.MultiplicativeLR(optimizer, lambda1)\n",
    "\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095\n",
      "0.09025\n",
      "0.0857375\n",
      "0.08145062499999998\n",
      "0.07737809374999999\n"
     ]
    }
   ],
   "source": [
    "# Each time factor \"0.95\" will be multiplied to the last learning rate\n",
    "for epoch in range(5):\n",
    "    # loss.backward()\n",
    "    optimizer.step()\n",
    "    # validate(...)\n",
    "    scheduler.step()\n",
    "    print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StepLR\n",
    "- Decays the learning rate of each parameter group by gamma every step_size epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "# Assuming optimizer uses lr = 0.05 for all groups\n",
    "# lr = 0.05     if epoch < 30\n",
    "# lr = 0.005    if 30 <= epoch < 60\n",
    "# lr = 0.0005   if 60 <= epoch < 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiStepLR\n",
    "- Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming optimizer uses lr = 0.05 for all groups\n",
    "# lr = 0.05     if epoch < 30\n",
    "# lr = 0.005    if 30 <= epoch < 80\n",
    "# lr = 0.0005   if epoch >= 80\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExponentialLR\n",
    "- Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CosineAnnealingLR\n",
    "### ReduceLROnPlateau\n",
    "- Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "for epoch in range(10):\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    # Note that step should be called after validate()\n",
    "    scheduler.step(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efbb18afce9a869a9a4bae09aee859ffdefe2f78fde0e8f74cb5cd933b79121d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
